{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Datasets Creation üöÄü§ñ\n",
    "\n",
    "This Colab notebook has the functionality to create the supervised fine tuning and preferences dataset for training a Constitutional AI agent.\n",
    "\n",
    "**Constitutional AI (CAI)** is a concept introduced by Anthropic in their paper. It is a method aimed at aligning AI systems with human values and ethical principles, particularly harmlessnes. CAI involves training AI models to follow a set of predefined rules or \"constitution\" that guides their behavior. This approach is particularly useful for practical settings where ensuring the AI's alignment with human values is crucial.\n",
    "\n",
    "**Why is this separate from training?** Why is this separate from training? Google Colab's RAM gets consumed too quickly when loading base datasets and processing models simultaneously. Separating these steps allows you to generate datasets first (you can reuse the ones uploaded in the repository) and then run the pipeline to train the models efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Prerequisites üìã‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/MarinaFuster/cai-implementation\n",
    "%cd cai-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and Environment üìã‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure root logger to display logs in Colab\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO, \n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# this is required for the code to be able to import the modules\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets üõ†Ô∏èüë®‚Äçüíº\n",
    "\n",
    "In this section, we will initialize the `DatasetManager`, which is responsible for creating the datasets for both the supervised fine tuning and direct preference optimization stages, and then we will create smaller dataset for the supervised fine tuning and direct preference optimization stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marin\\Documents\\GitHub\\cai-implementation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-08 12:55:25,298 - INFO - PyTorch version 2.6.0 available.\n",
      "2025-02-08 12:55:32,337 - INFO - Initialized DatasetManager.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from src import DatasetManager\n",
    "\n",
    "datasets_output_dir = Path(os.getenv('DATASETS_OUTPUT_DIR'))\n",
    "datasets_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "dataset_manager = DatasetManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = dataset_manager.create_sft_train_dataset(\n",
    "    n_samples_harmless=3000, \n",
    "    n_samples_helpful=800,\n",
    "    seed=42,\n",
    "    store=True,\n",
    "    output_dataset_path=datasets_output_dir.joinpath('sft_dataset'))\n",
    "print(f'Created SFT dataset, stored in {datasets_output_dir.joinpath(\"sft_dataset\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs_dataset = dataset_manager.create_prefs_train_dataset(\n",
    "    n_samples_harmless=3000, \n",
    "    n_samples_helpful=800,\n",
    "    seed=42,\n",
    "    store=True,\n",
    "    output_dataset_path=datasets_output_dir.joinpath('prefs_dataset'))\n",
    "print(f'Created preferences dataset, stored in {datasets_output_dir.joinpath(\"prefs_dataset\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
