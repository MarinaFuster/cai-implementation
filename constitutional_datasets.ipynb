{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Datasets Creation üöÄü§ñ\n",
    "\n",
    "This Colab notebook has the functionality to create the supervised fine tuning and preferences dataset for training a Constitutional AI agent.\n",
    "\n",
    "**Constitutional AI (CAI)** is a concept introduced by Anthropic in their paper. It is a method aimed at aligning AI systems with human values and ethical principles, particularly harmlessnes. CAI involves training AI models to follow a set of predefined rules or \"constitution\" that guides their behavior. This approach is particularly useful for practical settings where ensuring the AI's alignment with human values is crucial.\n",
    "\n",
    "**Why is this separate from training?** Why is this separate from training? Google Colab's RAM gets consumed too quickly when loading base datasets and processing models simultaneously. Separating these steps allows you to generate datasets first (you can reuse the ones uploaded in the repository) and then run the pipeline to train the models efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Prerequisites üìã‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/MarinaFuster/cai-implementation\n",
    "%cd cai-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and Environment üìã‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure root logger to display logs in Colab\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO, \n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# this is required for the code to be able to import the modules\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets üõ†Ô∏èüë®‚Äçüíº\n",
    "\n",
    "In this section, we will initialize the `DatasetManager`, which is responsible for creating the datasets for both the supervised fine tuning and direct preference optimization stages, and then we will create smaller dataset for the supervised fine tuning and direct preference optimization stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marin\\Documents\\GitHub\\cai-implementation\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-08 12:55:25,298 - INFO - PyTorch version 2.6.0 available.\n",
      "2025-02-08 12:55:32,337 - INFO - Initialized DatasetManager.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from src import DatasetManager\n",
    "\n",
    "datasets_output_dir = Path(os.getenv('DATASETS_OUTPUT_DIR'))\n",
    "datasets_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "dataset_manager = DatasetManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = dataset_manager.create_sft_train_dataset(\n",
    "    n_samples_harmless=3000, \n",
    "    n_samples_helpful=800,\n",
    "    seed=42,\n",
    "    store=True,\n",
    "    output_dataset_path=datasets_output_dir.joinpath('sft_dataset'))\n",
    "print(f'Created SFT dataset, stored in {datasets_output_dir.joinpath(\"sft_dataset\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 12:55:43,935 - INFO - Loaded harmless dataset: HuggingFaceH4/cai-conversation-harmless.\n",
      "2025-02-08 12:55:44,107 - INFO - Extracted 3000 inputs and 2 outputs.\n",
      "c:\\Users\\marin\\Documents\\GitHub\\cai-implementation\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\marin\\.cache\\huggingface\\hub\\datasets--HuggingFaceH4--ultrafeedback_binarized. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train_prefs split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61135/61135 [00:01<00:00, 58426.78 examples/s]\n",
      "Generating train_sft split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61135/61135 [00:00<00:00, 65701.57 examples/s]\n",
      "Generating test_prefs split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 45747.64 examples/s]\n",
      "Generating test_sft split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 43035.28 examples/s]\n",
      "Generating train_gen split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61135/61135 [00:00<00:00, 96814.96 examples/s] \n",
      "Generating test_gen split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 63557.76 examples/s]\n",
      "2025-02-08 12:58:16,711 - INFO - Loaded helpful dataset: HuggingFaceH4/ultrafeedback_binarized.\n",
      "2025-02-08 12:58:16,882 - INFO - Extracted 800 inputs and 2 outputs.\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3800/3800 [00:00<00:00, 401550.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created preferences dataset, stored in datasets\\prefs_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prefs_dataset = dataset_manager.create_prefs_train_dataset(\n",
    "    n_samples_harmless=3000, \n",
    "    n_samples_helpful=800,\n",
    "    seed=42,\n",
    "    store=True,\n",
    "    output_dataset_path=datasets_output_dir.joinpath('prefs_dataset'))\n",
    "print(f'Created preferences dataset, stored in {datasets_output_dir.joinpath(\"prefs_dataset\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
