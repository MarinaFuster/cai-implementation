{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Datasets Creation üöÄü§ñ\n",
    "\n",
    "This Colab notebook has the functionality to create the supervised fine tuning and preferences dataset for training a Constitutional AI agent.\n",
    "\n",
    "**Constitutional AI (CAI)** is a concept introduced by Anthropic in their paper. It is a method aimed at aligning AI systems with human values and ethical principles, particularly harmlessnes. CAI involves training AI models to follow a set of predefined rules or \"constitution\" that guides their behavior. This approach is particularly useful for practical settings where ensuring the AI's alignment with human values is crucial.\n",
    "\n",
    "**Why is this separate from training?** Why is this separate from training? Google Colab's RAM gets consumed too quickly when loading base datasets and processing models simultaneously. Separating these steps allows you to generate datasets first (you can reuse the ones uploaded in the repository) and then run the pipeline to train the models efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/MarinaFuster/cai-implementation\n",
    "%cd cai-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# this is required for the code to be able to import the modules\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset Manager üõ†Ô∏èüë®‚Äçüíº\n",
    "\n",
    "In this section, we will initialize the `DatasetManager`, which is responsible for creating the datasets for both the supervised fine tuning and direct preference optimization stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import DatasetManager\n",
    "dataset_manager = DatasetManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "datasets_output_dir = Path(os.getenv('DATASETS_OUTPUT_DIR'))\n",
    "datasets_output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = dataset_manager.create_sft_train_dataset(\n",
    "    n_samples_harmless=3000, \n",
    "    n_samples_helpful=800,\n",
    "    seed=42,\n",
    "    store=True,\n",
    "    output_dataset_path=datasets_output_dir.joinpath('sft_dataset'))\n",
    "print(f'Created SFT dataset, stored in {datasets_output_dir.joinpath('sft_dataset')}')\n",
    "print(sft_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs_dataset = dataset_manager.create_prefs_train_dataset(\n",
    "    n_samples_harmless=3000, \n",
    "    n_samples_helpful=800,\n",
    "    seed=42,\n",
    "    store=True,\n",
    "    output_dataset_path=datasets_output_dir.joinpath('prefs_dataset'))\n",
    "print(f'Created preferences dataset, stored in {datasets_output_dir.joinpath('prefs_dataset')}')\n",
    "print(prefs_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
