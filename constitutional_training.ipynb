{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Training Pipeline üöÄü§ñ\n",
    "\n",
    "This Colab notebook has the functionality to run the entire constitutional training setup.\n",
    "\n",
    "**Constitutional AI (CAI)** is a concept introduced by Anthropic in their paper. It is a method aimed at aligning AI systems with human values and ethical principles, particularly harmlessnes. CAI involves training AI models to follow a set of predefined rules or \"constitution\" that guides their behavior. This approach is particularly useful for practical settings where ensuring the AI's alignment with human values is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üìã‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/MarinaFuster/cai-implementation\n",
    "%cd cai-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure root logger to display logs in Colab\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO, \n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# this is required for the code to be able to import the modules\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Managers üõ†Ô∏èüë®‚Äçüíº\n",
    "\n",
    "In this section, we will initialize the `DatasetManager`, which is responsible for creating the datasets for both the supervised fine tuning and direct preference optimization stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import DatasetManager\n",
    "dataset_manager = DatasetManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Finetuning Stage üéØüìö\n",
    "\n",
    "In this stage, we fine-tune the pre-trained model using a labeled dataset. The goal is to improve the model's performance on specific tasks by providing it with examples of the correct output for given inputs. This process helps the model learn to make more accurate predictions and better align with the desired outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "sft_output_dir = Path(os.getenv('SFT_OUTPUT_DIR'))\n",
    "sft_output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import ModelManager\n",
    "model_manager = ModelManager(model_name=\"mistralai/Mistral-7B-Instruct-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = dataset_manager.get_sft_train_dataset(n_samples_harmless=2000, n_samples_helpful=800)\n",
    "tokenized_sft_dataset = sft_dataset.map(model_manager.tokenize_function, batched=True)\n",
    "tokenized_sft_dataset = tokenized_sft_dataset.remove_columns([\"input_text\", \"output_text\"])\n",
    "tokenized_sft_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=sft_output_dir,\n",
    "    per_device_train_batch_size=1,  # Keep batch size low to fit in memory\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients over multiple steps\n",
    "    num_train_epochs=3,  # Adjust based on dataset size\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-4,  # Adjust based on performance\n",
    "    fp16=True,  # Enable mixed precision for speed\n",
    "    optim=\"paged_adamw_8bit\",  # More memory-efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=\"none\"  # Disable wandb integration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to key layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "base_model = get_peft_model(model_manager.model, lora_config)\n",
    "base_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_sft_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "base_model.save_pretrained(sft_output_dir)\n",
    "model_manager.tokenizer.save_pretrained(sft_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Preference Optimization Stage üéØüîç\n",
    "\n",
    "In this stage, we optimize the model based on direct user preferences. The goal is to align the model's behavior with the preferences and values of the users by using feedback directly from them. This process involves collecting user feedback on the model's outputs and using this information to adjust the model's parameters, ensuring that it produces results that are more in line with what users want and expect. This stage is crucial for creating AI systems that are not only accurate but also user-friendly and aligned with human values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "dpo_output_dir = Path(os.getenv('DPO_OUTPUT_DIR'))\n",
    "dpo_output_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import ModelManager\n",
    "sft_model_manager = ModelManager(model_dir=sft_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset = dataset_manager.get_prefs_train_dataset(n_samples_harmless=2000, n_samples_helpful=800)\n",
    "tokenized_dpo_dataset = dpo_dataset.map(sft_model_manager.tokenize_function, batched=True)\n",
    "tokenized_dpo_dataset = tokenized_dpo_dataset.remove_columns([\"input_text\", \"output_text\"])\n",
    "tokenized_dpo_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "dpo_training_args = DPOConfig(\n",
    "    output_dir=dpo_output_dir, \n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Initialize the DPOTrainer using the fine-tuned model as the starting point\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=sft_model_manager.model,  \n",
    "    ref_model=None,\n",
    "    args=dpo_training_args,  \n",
    "    train_dataset=dpo_dataset,  # Your prepared DPO dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.train()\n",
    "sft_model_manager.model.save_pretrained(dpo_output_dir)\n",
    "sft_model_manager.tokenizer.save_pretrained(dpo_output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
