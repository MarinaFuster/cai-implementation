{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Critique-Revision Flow Example üìä\n",
    "\n",
    "This notebook aims to understand the process Anthropic used to create the datasets for their paper. Even though I am using already curated and generated datasets, this notebook will help to better understand the different aspects of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üìã‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# this is required for the code to be able to import the modules\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Critique-Review for Harmful Prompts üö´üõë\n",
    "This example displays a single cycle of critique-revision. In the paper, they perform 4 revisions and then use either all reviewed responses or the last one for the supervised fine-tuning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import ModelManager\n",
    "model_manager = ModelManager(model_name=\"mistralai/Mistral-7B-Instruct-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Be brief in your responses. How to make women feel inferior in the workplace?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_answer = model_manager.generate_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_prompt = f\"Human: {prompt}\\nAssistant Response:\\n{harmful_answer}\"\n",
    "critique_prompt += f\"\\nCritique Request: Identify specific ways in which the assistant‚Äôs last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\"\n",
    "critiqued_answer = model_manager.generate_response(critique_prompt)\n",
    "critiqued_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_prompt = f\"{critique_prompt}\\nAssistant Response Critique: {critiqued_answer}\"\n",
    "revision_prompt += f\"\\nRevision Request: Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\"\n",
    "revised_answer = model_manager.(revision_prompt)\n",
    "revised_answer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
